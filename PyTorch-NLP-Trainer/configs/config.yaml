# Working directory
work_dir: 'work_space'

# Model architecture
net_type: 'Transformer'
num_layers: 6  # Number of transformer layers
num_heads: 8  # Number of attention heads
kernel_size: [2, 3, 4, 5]  # Keep this for compatibility with the framework

# Embedding parameters
embed_size: 512  # Embedding dimension
context_size: 512  # Max sequence length
dropout: 0.1  # Standard transformer dropout rate

# Training parameters
batch_size: 16  # Smaller batch size for transformer
num_epochs: 100
lr: 0.0001  # Lower learning rate for stability
optim_type: 'Adam'
weight_decay: 0.01  # Higher weight decay for regularization

# Learning rate scheduler
milestones: [30, 60, 90]  # LR decay points
gamma: 0.1  # LR decay factor

# Loss function
loss_type: 'CrossEntropyLoss'

# Data parameters
data_type: 'text_dataset'
train_data: 'data/dataset/train'
test_data: 'data/dataset/test'
vocab_file: 'data/dataset/vocab.txt'
class_name: 'data/dataset/class_name.txt'
num_workers: 4
resample: false

# Evaluation
topk: [1, 3, 5]
log_freq: 5
finetune: false
flag: train
gpu_id: [0]